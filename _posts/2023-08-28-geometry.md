---
layout: post
title: "The geometry of diffusion guidance"
description: "More thoughts on diffusion guidance, with a focus on its geometry in the input space."

tags: [diffusion, score function, deep learning, generative models, guidance, geometry, vectors]

image:
  feature: geometry.jpg
comments: true
share: true
---

Guidance is a powerful method that can be used to enhance diffusion model sampling. As I've discussed in [an earlier blog post](https://sander.ai/2022/05/26/guidance.html), it's almost like a cheat code: it can improve sample quality so much that it's as if the model had ten times the number of parameters -- an order of magnitude improvement, basically for free! This follow-up post provides a geometric interpretation and visualisation of the diffusion sampling procedure, which I've found particularly useful to explain how guidance works.

## <a name="warning"> A word of warning about high-dimensional spaces

<figure>
  <a href="/images/dimensions.jpg"><img src="/images/dimensions.jpg"></a>
</figure>


Sampling algorithms for diffusion models typically start by initialising a _canvas_ with random noise, and then repeatedly updating this canvas based on model predictions, until a sample from the model distribution eventually emerges.

<!-- TODO: image sequence starting from noise and then showing the real image at the end -->

We will represent this canvas by a vector $$\mathbf{x}_t$$, where $$t$$ represents the current time step in the sampling procedure. By convention, the diffusion process which gradually corrupts inputs into random noise moves forward in time from $$t=0$$ to $$t=T$$, so the sampling procedure goes backward in time, from $$t=T$$ to $$t=0$$. Therefore $$\mathbf{x}_T$$ corresponds to random noise, and $$\mathbf{x}_0$$ corresponds to a sample from the data distribution.

**$$\mathbf{x}_t$$ is a high-dimensional vector**: for example, if a diffusion model produces images of size 64x64, there are 12,288 different scalar intensity values (3 colour channels per pixel). The sampling procedure then traces a path through a 12,288-dimensional Euclidean space.

It's pretty difficult for the human brain to comprehend what that actually looks like in practice. Because our intuition is firmly rooted in our 3D surroundings, it actually tends to fail us in surprising ways in high-dimensional spaces. A while back, I wrote [a blog post](https://sander.ai/2020/09/01/typicality.html) about some of the implications for high-dimensional probability distributions in particular. [This note about why high-dimensional spheres are "spikey"](http://www.penzba.co.uk/cgi-bin/PvsNP.py?SpikeySpheres#HN2) is also worth a read, if you quickly want to get a feel for how weird things can get. A more thorough treatment of high-dimensional geometry can be found in chapter 2 of 'Foundations of Data Science'[^foundations] by Blum, Hopcroft and Kannan, which is [available to download in PDF format](https://www.cs.cornell.edu/jeh/book.pdf).

Nevertheless, in this blog post, **I will use diagrams that represent $$\mathbf{x}_t$$ in _two_ dimensions**, because unfortunately that's all the spatial dimensions available on your screen. **This is dangerous**: following our intuition in 2D might lead us to the wrong conclusions. But I'm going to do it anyway, because in spite of this, I've found these diagrams quite helpful to explain how manipulations such as guidance affect diffusion sampling in practice.

Here's some advice from Geoff Hinton on dealing with high-dimensional spaces that may or may not help:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I&#39;m laughing so hard at this slide a friend sent me from one of Geoff Hinton&#39;s courses;<br><br>&quot;To deal with hyper-planes in a 14-dimensional space, visualize a 3-D space and say &#39;fourteen&#39; to yourself very loudly. Everyone does it.&quot; <a href="https://t.co/nTakZArbsD">pic.twitter.com/nTakZArbsD</a></p>&mdash; Robbie Barrat (@videodrome) <a href="https://twitter.com/videodrome/status/1005887240407379969?ref_src=twsrc%5Etfw">June 10, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

... anyway, **you've been warned!**

## <a name="sampling"></a> Visualising diffusion sampling

<figure>
  <a href="/images/dice.jpg"><img src="/images/dice.jpg"></a>
</figure>

To start off, let's visualise what a step of diffusion sampling typically looks like. I will use a real photograph to which I've added varying amounts of noise to stand in for intermediate samples in the diffusion sampling process:

<figure>
  <a href="/images/noisy_bundle_128.png"><img src="/images/noisy_bundle_128.png" alt="Bundle the bunny, with varying amounts of noise added."></a>
  <figcaption>Bundle the bunny, with varying amounts of noise added. <a href="https://twitter.com/kipperrii/status/1574557416741474304">Photo credit: kipply</a>.</figcaption>
</figure>

During diffusion model training, examples of noisy images are produced by taking examples of clean images from the data distribution, and adding varying amounts of noise to them. This is what I've done above. During sampling, we start from a canvas that is pure noise, and then the model gradually removes random noise and replaces it with meaningful structure in accordance with the data distribution. Note that I will be using this set of images to represent intermediate samples from a model, even though that's not how they were constructed. If the model is good enough, you shouldn't be able to tell the difference anyway!

In the diagram below, we have an intermediate noisy sample $$\mathbf{x}_t$$, somewhere in the middle of the sampling process, as well as the final output of that process $$\mathbf{x}_0$$, which is noise-free:

<figure>
  <a href="/images/geometry_diagram001.png"><img src="/images/geometry_diagram001.png" style="border: 1px dotted #bbb;" alt="Diagram showing an intermediate noisy sample, as well as the final output of the sampling process."></a>
  <figcaption>Diagram showing an intermediate noisy sample, as well as the final output of the sampling process.</figcaption>
</figure>

Imagine the two spatial dimensions of your screen representing just two of many thousands of pixel colour intensities (red, green or blue). Different spatial positions in the diagram correspond to different images. A single step in the sampling procedure is taken by using the model to **predict where the final sample will end up**. We'll call this prediction $$\hat{\mathbf{x}}_0$$:

<figure>
  <a href="/images/geometry_diagram002.png"><img src="/images/geometry_diagram002.png" style="border: 1px dotted #bbb;" alt="Diagram showing the prediction of the final sample from the current step in the sampling process."></a>
  <figcaption>Diagram showing the prediction of the final sample from the current step in the sampling process.</figcaption>
</figure>

Note how this prediction is roughly in the direction of $$\mathbf{x}_0$$, but we're not able to predict $$\mathbf{x}_0$$ exactly from the current point in the sampling process, $$\mathbf{x}_t$$, because the noise obscures a lot of information (especially fine-grained details), which we aren't able to fill in all in one go. Indeed, if we were, there would be no point to this iterative sampling procedure: we could just go directly from pure noise $$\mathbf{x}_T$$ to a clean image $$\mathbf{x}_0$$ in one step. (As an aside, this is more or less what Consistency Models[^cm] try to achieve.)

**Diffusion models estimate the _expectation_ of $$\mathbf{x}_0$$**, given the current noisy input $$\mathbf{x}_t$$: $$\hat{\mathbf{x}}_0 = \mathbb{E}[\mathbf{x}_0 \mid \mathbf{x}_t]$$. At the highest noise levels, this expectation basically corresponds to the mean of the entire dataset, because very noisy inputs are not very informative. As a result, the prediction $$\hat{\mathbf{x}}_0$$ will look like a very blurry image when visualised. At lower noise levels, this prediction will become sharper and sharper, and it will eventually resemble a sample from the data distribution. In [a previous blog post](https://sander.ai/2023/07/20/perspectives.html#expectation), I go into a little bit more detail about why diffusion models end up estimating expectations.

In practice, diffusion models are often parameterised to predict noise, rather than clean input, which I also discussed in [the same blog post](https://sander.ai/2023/07/20/perspectives.html#conventions). Some models also predict time-dependent linear combinations of the two. Long story short, all of these parameterisations are equivalent once the model has been trained, because a prediction of one of these quantities can be turned into a prediction of another through a linear combination of the prediction itself and the noisy input $$\mathbf{x}_t$$. That's why we can always get a prediction $$\hat{\mathbf{x}}_0$$ out of any diffusion model, regardless of how it was parameterised or trained: for example, if the model predicts the noise, simply take the noisy input and subtract the predicted noise.

Diffusion model predictions also correspond to an estimate of the so-called *score function*, $$\nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t)$$. This can be interpreted as the direction in input space along which the log-likelihood of the input increases maximally. In other words, it's the answer to the question: **"how should I change the input to make it more likely?"**
y s
Diffusion sampling now proceeds by **taking a small step in the direction of this prediction**:

<figure>
  <a href="/images/geometry_diagram003.png"><img src="/images/geometry_diagram003.png" style="border: 1px dotted #bbb;" alt="Diagram showing how we take a small step in the direction of the prediction of the final sample."></a>
  <figcaption>Diagram showing how we take a small step in the direction of the prediction of the final sample.</figcaption>
</figure>

This should look familiar to any machine learning practitioner, as it's very similar to neural network training via gradient descent: backpropagation gives us the direction of steepest descent at the current point in parameter space, and at each optimisation step, we take a small step in that direction. Taking a very large step wouldn't get us anywhere interesting, because the estimated direction is only valid locally. The same is true for diffusion sampling, except we're now operating in the input space, rather than in the space of model parameters.

What happens next depends on the specific sampling algorithm we've chosen to use. There are many to choose from: DDPM[^ddpm] (also called ancestral sampling), DDIM[^ddim], DPM++[^dpmpp] and ODE-based sampling[^sde] (with many sub-variants using different ODE solvers) are just a few examples. Some of these algorithms are deterministic, which means the only source of randomness in the sampling procedure is the initial noise on the canvas. Others are stochastic, which means that further noise is injected at each step of the sampling procedure.

We'll use DDPM as an example, because it is one of the oldest and most commonly used sampling algorithms for diffusion models. This is a stochastic algorithm, so **some random noise is added after taking a step** in the direction of the model prediction:

<figure>
  <a href="/images/geometry_diagram004.png"><img src="/images/geometry_diagram004.png" style="border: 1px dotted #bbb;" alt="Diagram showing how noise is added after taking small step in the direction of the model prediction."></a>
  <figcaption>Diagram showing how noise is added after taking small step in the direction of the model prediction.</figcaption>
</figure>

Note that I am intentionally glossing over some of the details of the sampling algorithm here (for example, the exact variance of the noise $$\varepsilon$$ that is added at each step). The diagrams are schematic and the focus is on building intuition, so I think I can get away with that, but obviously it's pretty important to get this right when you actually want to implement this algorithm.

For deterministic sampling algorithms, we can simply skip this step (i.e. set $$\varepsilon = 0$$). After this, we end up in $$\mathbf{x}_{t-1}$$, which is the next iterate in the sampling procedure, and should correspond to a slightly less noisy sample. **To proceed, we rinse and repeat**. We can again make a prediction $$\hat{\mathbf{x}}_0$$: 

<figure>
  <a href="/images/geometry_diagram005.png"><img src="/images/geometry_diagram005.png" style="border: 1px dotted #bbb;" alt="Diagram showing the updated prediction of the final sample from the current step in the sampling process."></a>
  <figcaption>Diagram showing the updated prediction of the final sample from the current step in the sampling process.</figcaption>
</figure>

Because we are in a different point in input space, this prediction will also be different. Concretely, **as the input to the model is now slightly less noisy, the prediction will be slightly less blurry**. We now take a small step in the direction of this new prediction, and add noise to end up in $$\mathbf{x}_{t-2}$$:

<figure>
  <a href="/images/geometry_diagram006.png"><img src="/images/geometry_diagram006.png" style="border: 1px dotted #bbb;" alt="Diagram showing a sequence of two DDPM sampling steepest."></a>
  <figcaption>Diagram showing a sequence of two DDPM sampling steps.</figcaption>
</figure>

We can keep doing this until we eventually reach $$\mathbf{x}_0$$, and we will have drawn a sample from the diffusion model. To summarise, below is an animated version of the above set of diagrams, showing the sequence of steps:

<figure>
  <a href="/images/geometry_diagram007.gif"><img src="/images/geometry_diagram007.gif" style="border: 1px dotted #c10;" alt="Animation of the above set of diagrams."></a>
  <figcaption>Animation of the above set of diagrams.</figcaption>
</figure>

## <a name="classifier-guidance"></a> Classifier guidance

<figure>
  <a href="/images/sorted.jpg"><img src="/images/sorted.jpg"></a>
</figure>

Classifier guidance[^sde] [^equilibrium] [^beatgans] provides a way to **steer diffusion sampling in the direction that maximises the probability of the final sample being classified as a particular class**. More broadly, this can be used to make the sample reflect any sort of conditioning signal that wasn't provided to the diffusion model during training. In other words, it enables *post-hoc* conditioning.

For classifier guidance, we need an auxiliary model that predicts $$p(y \mid \mathbf{x})$$, where $$y$$ represents an arbitrary input feature, which could be a class label, a textual description of the input, or even a more structured object like a segmentation map or a depth map. We'll call this model a *classifier*, but keep in mind that we can use many different kinds of models for this purpose, not just classifiers in the narrow sense of the word. What's nice about this setup, is that such models are usually smaller and easier to train than diffusion models.

One important caveat is that we will be applying this auxiliary model to *noisy* inputs $$\mathbf{x}_t$$, at varying levels of noise, so it has to be robust against this particular type of input distortion. This seems to preclude the use of off-the-shelf classifiers, and implies that we need to train a custom noise-robust classifier, or at the very least, fine-tune an off-the-shelf classifier to be noise-robust. We can also explicitly condition the classifier on the time step $$t$$, so the level of noise does not have to be inferred from the input $$\mathbf{x}_t$$ alone.

However, it turns out that we can construct a reasonable noise-robust classifier by combining an off-the-shelf classifier (which expects noise-free inputs) with our diffusion model. Rather than applying the classifier to $$\mathbf{x}_t$$, we first predict $$\hat{\mathbf{x}}_0$$ with the diffusion model, and use that as input to the classifier instead. $$\hat{\mathbf{x}}_0$$ is still distorted, but by blurring rather than by Gaussian noise. Off-the-shelf classifiers tend to be much more robust to this kind of distortion out of the box. Bansal et al.[^universal] named this trick "forward universal guidance", though it has been known for some time. They also suggest some more advanced approaches for post-hoc guidance.

Using the classifier, we can now determine the direction in input space that maximises the log-likelihood of the conditioning signal, simply by computing **the gradient with respect to $$\mathbf{x}_t$$**: $$\nabla_{\mathbf{x}_t} \log p(y \mid \mathbf{x}_t)$$. (Note: if we used the above trick to construct a noise-robust classifier from an off-the-shelf one, this means we'll need to backpropagate through the diffusion model as well.)

<figure>
  <a href="/images/geometry_diagram008.png"><img src="/images/geometry_diagram008.png" style="border: 1px dotted #bbb;" alt="Diagram showing the update directions from the diffusion model and the classifier."></a>
  <figcaption>Diagram showing the update directions from the diffusion model and the classifier.</figcaption>
</figure>

To apply classifier guidance, we **combine the directions obtained from the diffusion model and from the classifier by adding them together**, and then we take a step in this combined direction instead:

<figure>
  <a href="/images/geometry_diagram009.png"><img src="/images/geometry_diagram009.png" style="border: 1px dotted #bbb;" alt="Diagram showing the combined update direction for classifier guidance."></a>
  <figcaption>Diagram showing the combined update direction for classifier guidance.</figcaption>
</figure>

As a result, the sampling procedure will trace a different trajectory through the input space. To control the influence of the conditioning signal on the sampling procedure, we can **scale the contribution of the classifier gradient by a factor $$\gamma$$**, which is called the *guidance scale*:

<figure>
  <a href="/images/geometry_diagram011.png"><img src="/images/geometry_diagram010.png" style="border: 1px dotted #bbb;" alt="Diagram showing the scaled classifier update direction."></a>
  <figcaption>Diagram showing the scaled classifier update direction.</figcaption>
</figure>

The combined update direction will then be influenced more strongly by the direction obtained from the classifier (provided that $$\gamma > 1$$, which is usually the case):

<figure>
  <a href="/images/geometry_diagram011.png"><img src="/images/geometry_diagram011.png" style="border: 1px dotted #bbb;" alt="Diagram showing the combined update direction for classifier guidance with guidance scale."></a>
  <figcaption>Diagram showing the combined update direction for classifier guidance with guidance scale.</figcaption>
</figure>

This scale factor $$\gamma$$ is an important sampling hyperparameter: if it's too low, the effect is negligible. If it's too high, the samples will be distorted and low-quality. This is because **gradients obtained from classifiers don't necessarily point in directions that lie on the image manifold** -- if we're not careful, we may actually end up in adversarial examples, which maximise the probability of the class label but don't actually look like an example of the class at all!

In [my previous blog post on diffusion guidance](https://sander.ai/2022/05/26/guidance.html), I made the connection between these operations on vectors in the input space, and the underlying manipulations of distributions they correspond to. It's worth briefly revisiting this connection to make it more apparent:

* We've taken the update direction obtained from the diffusion model, which corresponds to $$\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)$$ (i.e. the score function), and the (scaled) update direction obtained from the classifier, $$\gamma \cdot \nabla_{\mathbf{x}_t} \log p(y \mid \mathbf{x}_t)$$, and combined them simply by adding them together: $$\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) + \gamma \cdot \nabla_{\mathbf{x}_t} \log p(y \mid \mathbf{x}_t)$$.

* This expression corresponds to the gradient of the logarithm of $$p_t(\mathbf{x}_t) \cdot p(y \mid \mathbf{x}_t)^\gamma$$.

* In other words, we have effectively *reweighted* the model distribution, changing the probability of each input in accordance with the probability the classifier assigns to the desired class label.

* The guidance scale $$\gamma$$ corresponds to the *temperature* of the classifier distribution. A high temperature implies that inputs to which the classifier assigns high probabilities are upweighted more aggressively, relative to other inputs.

* The result is a new model that is much more likely to produce samples that align with the desired class label.

An animated diagram of a single step of sampling with classifier guidance is shown below:

<figure>
  <a href="/images/geometry_diagram018.gif"><img src="/images/geometry_diagram018.gif" style="border: 1px dotted #c10;" alt="Animation of a single step of sampling with classifier guidance."></a>
  <figcaption>Animation of a single step of sampling with classifier guidance.</figcaption>
</figure>

## <a name="classifier-free-guidance"></a> Classifier-free guidance

<figure>
  <a href="/images/winding_road.jpg"><img src="/images/winding_road.jpg"></a>
</figure>

Classifier-free guidance[^cf] is a variant of guidance that does not require an auxiliary classifier model. Instead, **a Bayesian classifier is constructed by combining a conditional and an unconditional generative model**.

Concretely, when training a conditional generative model $$p(\mathbf{x}\mid y)$$, we can drop out the conditioning $$y$$ some percentage of the time (usually 10-20%) so that the same model can also act as an unconditional generative model, $$p(\mathbf{x})$$. It turns out that this does not have a detrimental effect on conditional modelling performance. Using Bayes' rule, we find that $$p(y \mid \mathbf{x}) \propto \frac{p(\mathbf{x}\mid y)}{p(\mathbf{x})}$$, which gives us a way to turn our generative model into a classifier.

In diffusion models, we tend to express this in terms of score functions, rather than in terms of probability distributions. Taking the logarithm and then the gradient w.r.t. $$\mathbf{x}$$, we get $$\nabla_\mathbf{x} \log p(y \mid \mathbf{x}) = \nabla_\mathbf{x} \log p(\mathbf{x} \mid y) - \nabla_\mathbf{x} \log p(\mathbf{x})$$. In other words, to obtain the gradient of the classifier log-likelihood with respect to the input, all we have to do is subtract the unconditional score function from the conditional score function.

Substituting this expression into the formula for the update direction of classifier guidance, we obtain the following:

$$\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) + \gamma \cdot \nabla_{\mathbf{x}_t} \log p(y \mid \mathbf{x}_t)$$

$$= \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) + \gamma \cdot \left( \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid y) - \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t) \right) $$

$$= (1 - \gamma) \cdot \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) + \gamma \cdot \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \mid y) . $$

The update direction is now a linear combination of the unconditional and conditional score functions. It would be a convex combination if it were the case that $$\gamma \in [0, 1]$$, but in practice $$\gamma > 1$$ tends to be were the magic happens, so this is merely a *barycentric* combination. Note that $$\gamma = 0$$ reduces to the unconditional case, and $$\gamma = 1$$ reduces to the conditional (*unguided*) case.

How do we make sense of this geometrically? With our hybrid conditional/unconditional model, we can make two predictions $$\hat{\mathbf{x}}_0$$. These will be different, because the conditioning information may allow us to make a more accurate prediction:

<figure>
  <a href="/images/geometry_diagram012.png"><img src="/images/geometry_diagram012.png" style="border: 1px dotted #bbb;" alt="Diagram showing the conditional and unconditional predictions."></a>
  <figcaption>Diagram showing the conditional and unconditional predictions.</figcaption>
</figure>

Next, we determine the difference vector between these two predictions. As we showed earlier, this corresponds to the gradient direction provided by the implied Bayesian classifier:

<figure>
  <a href="/images/geometry_diagram013.png"><img src="/images/geometry_diagram013.png" style="border: 1px dotted #bbb;" alt="Diagram showing the difference vector obtained by subtracting the directions corresponding to the two predictions."></a>
  <figcaption>Diagram showing the difference vector obtained by subtracting the directions corresponding to the two predictions.</figcaption>
</figure>

We now scale this vector by $$\gamma$$:

<figure>
  <a href="/images/geometry_diagram014.png"><img src="/images/geometry_diagram014.png" style="border: 1px dotted #bbb;" alt="Diagram showing the amplified difference vector."></a>
  <figcaption>Diagram showing the amplified difference vector.</figcaption>
</figure>

Starting from the unconditional prediction for $$\hat{\mathbf{x}}_0$$, this vector points towards a new implicit prediction, which corresponds to a stronger influence of the conditioning signal. This is the prediction we will now take a small step towards:

<figure>
  <a href="/images/geometry_diagram014.png"><img src="/images/geometry_diagram015.png" style="border: 1px dotted #bbb;" alt="Diagram showing the direction to step in for classifier-free guidance."></a>
  <figcaption>Diagram showing the direction to step in for classifier-free guidance.</figcaption>
</figure>

Classifier-free guidance tends to work a lot better than classifier guidance, because the Bayesian classifier is much more robust than a separately trained one, and the resulting update directions are much less likely to be adversarial. On top of that, it doesn't require an auxiliary model, and generative models can be made compatible with classifier-free guidance simply through *conditioning dropout* during training. On the flip side, that means we can't use this for post-hoc conditioning -- all conditioning signals have to be available during training of the generative model itself. [My previous blog post on guidance](https://sander.ai/2022/05/26/guidance.html) covers the differences in more detail.

An animated diagram of a single step of sampling with classifier-free guidance is shown below:

<figure>
  <a href="/images/geometry_diagram019.gif"><img src="/images/geometry_diagram019.gif" style="border: 1px dotted #c10;" alt="Animation of a single step of sampling with classifier-free guidance."></a>
  <figcaption>Animation of a single step of sampling with classifier-free guidance.</figcaption>
</figure>

## <a name="closing-thoughts"></a> Closing thoughts

<figure>
  <a href="/images/trees_water.jpg"><img src="/images/trees_water.jpg"></a>
</figure>

What's surprising about guidance, in my opinion, is how powerful it is in practice, despite its relative simplicity. The modifications to the sampling procedure required to apply guidance are all **linear operations** on vectors in the input space. This is what makes it possible to interpret the procedure geometrically.

How can a set of linear operations affect the outcome of the sampling procedure so profoundly? The key is **iterative refinement**: these simple modifications are applied repeatedly, and crucially, they are interleaved with a very non-linear operation, which is the application of the diffusion model itself, to predict the next update direction. As a result, any linear modification of the update direction has a non-linear effect on the next update direction. Across many sampling steps, the resulting effect is highly non-linear and powerful: small differences in each step accumulate, and result in trajectories with very different endpoints.

I hope the visualisations in this post are a useful complement to [my previous writing on the topic of guidance](https://sander.ai/2022/05/26/guidance.html). Feel free to let me know your thoughts in the comments, or on Twitter/X (<a href="https://twitter.com/sedielem">@sedielem</a>) or Threads (<a href="https://www.threads.net/@sanderdieleman">@sanderdieleman</a>).


*If you would like to cite this post in an academic context, you can use this BibTeX snippet:*

```
@misc{dieleman2023geometry,
  author = {Dieleman, Sander},
  title = {The geometry of diffusion guidance},
  url = {https://sander.ai/2023/08/28/geometry.html},
  year = {2023}
}
```

## <a name="acknowledgements"></a> Acknowledgements

Thanks to Bundle for modelling and to kipply for permission to use [this photograph](https://twitter.com/kipperrii/status/1574557416741474304). Thanks to my colleagues at Google DeepMind for various discussions, which continue to shape my thoughts on this topic!

## <a name="references"></a> References

[^foundations]: Blum, Hopcroft, Kannan, "[Foundations of Data science](https://www.cs.cornell.edu/jeh/book.pdf)", Cambridge University Press, 2020

[^cm]: Song, Dhariwal, Chen, Sutskever, "[Consistency Models](https://arxiv.org/abs/2303.01469)", International Conference on Machine Learning, 2023.

[^ddpm]: Ho, Jain, Abbeel, "[Denoising Diffusion Probabilistic Models](https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html)", 2020.

[^ddim]: Song, Meng, Ermon, "[Denoising Diffusion Implicit Models](https://arxiv.org/abs/2010.02502)", International Conference on Learning Representations, 2021.

[^dpmpp]: Lu, Zhou, Bao, Chen, Li, Zhu, "[DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models](https://arxiv.org/abs/2211.01095)", arXiv, 2022.

[^sde]: Song, Sohl-Dickstein, Kingma, Kumar, Ermon and Poole, "[Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456)", International Conference on Learning Representations, 2021.

[^equilibrium]: Sohl-Dickstein, Weiss, Maheswaranathan and Ganguli, "[Deep Unsupervised Learning using Nonequilibrium Thermodynamics](https://arxiv.org/abs/1503.03585)", International Conference on Machine Learning, 2015.

[^beatgans]: Dhariwal, Nichol, "[Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)", Neural Information Processing Systems, 2021.

[^universal]: Bansal, Chu, Schwarzschild, Sengupta, Goldblum, Geiping, Goldstein, "[Universal Guidance for Diffusion Models](https://arxiv.org/abs/2302.07121)", Computer Vision and Pattern Recognition, 2023.

[^cf]: Ho, Salimans, "[Classifier-Free Diffusion Guidance](https://openreview.net/forum?id=qw8AKxfYbI)", NeurIPS workshop on DGMs and Applications", 2021.